{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Título: Conexion Spark con MongoDB\n",
    "##### Autor: Dr. Gabriel Guerrero, saxsa2000@gmail.com\n",
    "##### Fecha: 20190805\n",
    "##### Referencia: gg20190805_spark_mongo\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark conexión con MongoDB\n",
    "\n",
    "En este cuaderno jupyter muestra como leer una colección existente de mongodb con SparkSQL.\n",
    "\n",
    "El mecanismo tiene varios pasos. \n",
    "\n",
    "El primer paso es leer una colección de mongoDB y convertirla a un DataFrame de SparkSQL.\n",
    "\n",
    "El segundo paso es utilizar el DataFrame de Spark y ejecutar los  metódos del dataframe que se requieran en el procesamiento.\n",
    "\n",
    "El tercer paso es guardar el resultado obtenido en un nuevo DataFrame de Spark.\n",
    "\n",
    "En el cuarto paso se almacena el DataFrame en una coleccion mongodb.\n",
    "\n",
    "Para facilitar el acceso con Jupyter al cluster Spark, utilizamos livy server\n",
    "\n",
    "livy server es un servidor tipo REST para Spark\n",
    "\n",
    "![mongo_spark](spark_mongo.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Comandos mágicos de Spark Sparkmagic \n",
    "\n",
    "Utilizamos comandos magicos de Spark Sparkmagic (\"%%\") para ejecutar enunciados del shell de linux\n",
    "\n",
    "Sparkmagic es un conjunto de herramientas para el trabajo interactivo con clusters remotos de Spark por medio del servidor Livy, que es un servidor de tipo REST server para Spark, trabajando por medio de cuadernos Jupyter. \n",
    "\n",
    "Caracteristicas comandos mágicos de Spark (Sparkmagic) \n",
    "\n",
    "Run Spark code in multiple languages against any remote Spark cluster through Livy\n",
    "\n",
    "Automatic SparkContext (sc) and HiveContext (sqlContext) creation\n",
    "\n",
    "Easily execute SparkSQL queries with the %%sql magic\n",
    "\n",
    "Automatic visualization of SQL queries in the PySpark, Spark and SparkR kernels; use an easy visual interface to interactively construct visualizations, no code required\n",
    "\n",
    "Easy access to Spark application information and logs (%%info magic)\n",
    "    \n",
    "Ability to capture the output of SQL queries as Pandas dataframes to interact with other Python libraries (e.g. matplotlib)\n",
    "\n",
    "\n",
    "\n",
    "##### ¿Por qué utilizar  livy + sparkmagic?\n",
    "\n",
    "sparkmagic es un cliente del servidor livy que se utiliza en un cuaderno Jupyter.\n",
    "\n",
    "Se tiene una arquitectura en donde ejecutamos los enunciados utilizando un cuaderno Jupyter\n",
    "\n",
    "Cuando se escribe un código Spark en nuestro cliente local Jupyter, el programa sparkmagic lo manda ejecutar al cluster Spark por medio del servidor livy\n",
    "\n",
    "\n",
    "\n",
    "Using sparkmagic + Jupyter notebook, data scientists can use Spark from their own Jupyter notebook, which is running on their localhost.\n",
    "\n",
    "We don’t need any Spark configuration getting from the Spark cluster.\n",
    "\n",
    "So we can execute Spark job in a cluster like running on a local machine.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![livy_spark](livyServer_Spark.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Análisis de procesos java por medio del enunciado jps\n",
    "\n",
    "Analizamos los procesos java por medio del enunciado jps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11363 SecondaryNameNode\n",
      "11539 Master\n",
      "11044 NameNode\n",
      "18710 Jps\n",
      "11612 Worker\n",
      "11725 LivyServer\n",
      "11167 DataNode\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "jps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iniciar servicios Hadoop HDFS en caso de no estar activos\n",
    "\n",
    "En este ejercicio se utiliza Hadoop HDFS, por lo que requerimos iniciar el servicio si este no esta activo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting namenodes on [hostsaxsa]\n",
      "hostsaxsa: namenode running as process 11044. Stop it first.\n",
      "hostsaxsa: datanode running as process 11167. Stop it first.\n",
      "Starting secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: secondarynamenode running as process 11363. Stop it first.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "start-dfs.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iniciar servicios Spark en caso de no estar activos\n",
    "\n",
    "En este ejercicio se utiliza Spark, por lo que requerimos iniciar el servicio si este no esta activo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "org.apache.spark.deploy.master.Master running as process 11539.  Stop it first.\n",
      "hostsaxsa: org.apache.spark.deploy.worker.Worker running as process 11612.  Stop it first.\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "\n",
    "start-spark.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Análisis Status mongoDB\n",
    "\n",
    "En caso de no estar activo el servidor MongoDB, debemos iniciarlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "● mongod.service - High-performance, schema-free document-oriented database\n",
      "   Loaded: loaded (/usr/lib/systemd/system/mongod.service; enabled; vendor preset: disabled)\n",
      "   Active: active (running) since lun 2019-08-05 13:17:26 CDT; 8h ago\n",
      "     Docs: https://docs.mongodb.org/manual\n",
      "  Process: 1169 ExecStart=/usr/bin/mongod $OPTIONS (code=exited, status=0/SUCCESS)\n",
      "  Process: 1122 ExecStartPre=/usr/bin/chmod 0755 /var/run/mongodb (code=exited, status=0/SUCCESS)\n",
      "  Process: 1098 ExecStartPre=/usr/bin/chown mongod:mongod /var/run/mongodb (code=exited, status=0/SUCCESS)\n",
      "  Process: 1081 ExecStartPre=/usr/bin/mkdir -p /var/run/mongodb (code=exited, status=0/SUCCESS)\n",
      " Main PID: 1496 (mongod)\n",
      "    Tasks: 23\n",
      "   CGroup: /system.slice/mongod.service\n",
      "           └─1496 /usr/bin/mongod -f /etc/mongod.conf\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "systemctl status mongod\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verificación del contenido de la base en mongoDB que deseamos consultar\n",
    "\n",
    "=== Ejecutar mongodb en una terminal Linux ===\n",
    "\n",
    "mongo\n",
    "\n",
    "=== Una vez iniciado el cliente MongoDB, mostrar bases en mongodb ===\n",
    "\n",
    "show dbs\n",
    "\n",
    "=== Usar la base de datos llamada bimbodb ===\n",
    "\n",
    "use bimbodb\n",
    "\n",
    "=== Mostrar colecciones de la base de datos ===\n",
    "\n",
    "show collections\n",
    "\n",
    "=== Mostrar el contenido de las colecciones ===\n",
    "\n",
    "db.cliente.find()\n",
    "\n",
    "db.producto.find()\n",
    "\n",
    "db.town_state.find().pretty()\n",
    "\n",
    "db.train.find().pretty()\n",
    "\n",
    "=== Salir de mongodb ===\n",
    "\n",
    "exit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "livy’s Default port number is 8998"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"from\":0,\"total\":0,\"sessions\":[]}"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\r",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r",
      "100    34  100    34    0     0   3777      0 --:--:-- --:--:-- --:--:--  3777\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "curl localhost:8998/sessions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Análisis de datos almacenados en MongoDB de datos abiertos de BIMBO\n",
    "\n",
    "Las fuentes de información son datos abiertos de BIMBO, los cuales se obtuvieron de un concurso de kaggle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>4</td><td>None</td><td>pyspark3</td><td>idle</td><td></td><td></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'20190805221113'"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "# current date and time\n",
    "now = datetime.now()\n",
    "timestamp = datetime.timestamp(now)\n",
    "date_time = now.strftime(\"%Y%m%d%H%M%S\")\n",
    "timestamp\n",
    "now\n",
    "date_time "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " pyspark.sql.functions.count(col)\n",
    "\n",
    "    Aggregate function: returns the number of items in a group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark SQL also includes a data source that can read data from other databases using JDBC.\n",
    "\n",
    "This is because the results are returned as a DataFrame and they can easily be processed in Spark SQL or joined with other data sources. \n",
    "\n",
    "To get started you will need to include the JDBC driver for your particular database "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read from a collection called \"cliente\" en una base en MongoDB llamada \"bimbodb\", especificamos \"bombodb.cliente\" en la entrada de la opcion \"URI option\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lectura de la base bimbodb y la coleccion cliente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cliente = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\")\\\n",
    "    .option(\"spark.mongodb.input.uri\", \"mongodb://127.0.0.1/bimbodb.cliente?readPreference=primaryPreferred\")\\\n",
    "    .option(\"spark.mongodb.output.uri\", \"mongodb://127.0.0.1/bimbodb.cliente\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Cliente_ID: integer (nullable = true)\n",
      " |-- NombreCliente: string (nullable = true)\n",
      " |-- _id: struct (nullable = true)\n",
      " |    |-- oid: string (nullable = true)"
     ]
    }
   ],
   "source": [
    "df_cliente.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[Cliente_ID: int, NombreCliente: string, _id: struct<oid:string>]"
     ]
    }
   ],
   "source": [
    "df_cliente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450000"
     ]
    }
   ],
   "source": [
    "df_cliente.select('Cliente_ID').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Cliente_ID: integer (nullable = true)\n",
      " |-- NombreCliente: string (nullable = true)"
     ]
    }
   ],
   "source": [
    "df_cliente_sin_id = df_cliente.drop('_id')\n",
    "df_cliente_sin_id.printSchema()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(Cliente_ID=4386922, NombreCliente='NO IDENTIFICADO'), Row(Cliente_ID=1982904, NombreCliente='LOS PEQUES'), Row(Cliente_ID=1960682, NombreCliente='MARIA DEL ROSARIO NARANJO NERI'), Row(Cliente_ID=183569, NombreCliente='LARIOS'), Row(Cliente_ID=163632, NombreCliente='CASA ZAMUDIO')]"
     ]
    }
   ],
   "source": [
    "df_cliente_sin_id.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+\n",
      "|Cliente_ID|       NombreCliente|\n",
      "+----------+--------------------+\n",
      "|   4386922|     NO IDENTIFICADO|\n",
      "|   1982904|          LOS PEQUES|\n",
      "|   1960682|MARIA DEL ROSARIO...|\n",
      "|    183569|              LARIOS|\n",
      "|    163632|        CASA ZAMUDIO|\n",
      "+----------+--------------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "df_cliente_sin_id.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+--------------------+\n",
      "|summary|        Cliente_ID|       NombreCliente|\n",
      "+-------+------------------+--------------------+\n",
      "|  count|            450000|              450000|\n",
      "|   mean|2349831.5673133335|   4103.354430379747|\n",
      "| stddev| 1915668.265326387|  22887.275797071834|\n",
      "|    min|                 2|056 THE AIRPORT M...|\n",
      "|    max|          19988629|                ÑEKA|\n",
      "+-------+------------------+--------------------+"
     ]
    }
   ],
   "source": [
    "df_cliente_sin_id.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "448852"
     ]
    }
   ],
   "source": [
    "df_cliente_sin_id.select('Cliente_ID').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1148"
     ]
    }
   ],
   "source": [
    "df_cliente_id_duplicados = df_cliente_sin_id.select('Cliente_ID').groupby('Cliente_ID')\\\n",
    "            .agg(count('Cliente_ID').alias('Apariciones'))\\\n",
    "            .sort('Apariciones', ascending=False)\\\n",
    "            .where(\"Apariciones != 1\")\n",
    "\n",
    "df_cliente_id_duplicados.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(Cliente_ID=6137009, Apariciones=2), Row(Cliente_ID=525200, Apariciones=2), Row(Cliente_ID=149688, Apariciones=2), Row(Cliente_ID=406654, Apariciones=2), Row(Cliente_ID=370466, Apariciones=2), Row(Cliente_ID=101541, Apariciones=2), Row(Cliente_ID=164940, Apariciones=2), Row(Cliente_ID=103066, Apariciones=2), Row(Cliente_ID=496802, Apariciones=2), Row(Cliente_ID=1453247, Apariciones=2)]"
     ]
    }
   ],
   "source": [
    "df_cliente_id_duplicados.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|Cliente_ID|Apariciones|\n",
      "+----------+-----------+\n",
      "|   6137009|          2|\n",
      "|    525200|          2|\n",
      "|    149688|          2|\n",
      "|    406654|          2|\n",
      "|    370466|          2|\n",
      "|    101541|          2|\n",
      "|    164940|          2|\n",
      "|    103066|          2|\n",
      "|    496802|          2|\n",
      "|   1453247|          2|\n",
      "+----------+-----------+\n",
      "only showing top 10 rows"
     ]
    }
   ],
   "source": [
    "df_cliente_id_duplicados.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+\n",
      "|Cliente_ID|NombreCliente          |\n",
      "+----------+-----------------------+\n",
      "|6137009   |F GDL S403 LAS HILAMAS |\n",
      "|6137009   |F  GDL S403 LAS HILAMAS|\n",
      "+----------+-----------------------+"
     ]
    }
   ],
   "source": [
    "\n",
    "df_cliente_sin_id.filter(\"Cliente_ID == 6137009\").show(2, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------------+\n",
      "|Cliente_ID|NombreCliente           |\n",
      "+----------+------------------------+\n",
      "|1453247   |NOVEDADES Y V  J  INGRID|\n",
      "|1453247   |NOVEDADES Y V J INGRID  |\n",
      "+----------+------------------------+"
     ]
    }
   ],
   "source": [
    "df_cliente_sin_id.filter(\"Cliente_ID == 1453247\").show(2, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cliente_sin_id.createOrReplaceTempView('cliente')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a88d0b02571746c7903eddc3add21693",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(HTML(value='Type:'), Button(description='Table', layout=Layout(width='70px'), st…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "976165973f6d46ebbe6523c412b25b13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%sql\n",
    "SELECT * FROM cliente WHERE NombreCliente LIKE '%MISCE%' limit 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lectura de la base bimbodb y la coleccion producto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_producto = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\")\\\n",
    "    .option(\"spark.mongodb.input.uri\", \"mongodb://127.0.0.1/bimbodb.producto?readPreference=primaryPreferred\")\\\n",
    "    .option(\"spark.mongodb.output.uri\", \"mongodb://127.0.0.1/bimbodb.producto\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- NombreProducto: string (nullable = true)\n",
      " |-- Producto_ID: integer (nullable = true)\n",
      " |-- _id: struct (nullable = true)\n",
      " |    |-- oid: string (nullable = true)"
     ]
    }
   ],
   "source": [
    "df_producto.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+------------------+\n",
      "|summary|      NombreProducto|       Producto_ID|\n",
      "+-------+--------------------+------------------+\n",
      "|  count|                2592|              2592|\n",
      "|   mean|                null|32591.095679012345|\n",
      "| stddev|                null|13004.091023722118|\n",
      "|    min|100pct Whole Whea...|                 0|\n",
      "|    max|Wonderbutter 680g...|             49997|\n",
      "+-------+--------------------+------------------+"
     ]
    }
   ],
   "source": [
    "df_producto_sin_id = df_producto.drop('_id')\n",
    "df_producto_sin_id.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_producto_sin_id.createOrReplaceTempView('producto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f488416de18407b9702ccc35c3452ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(HTML(value='Type:'), Button(description='Table', layout=Layout(width='70px'), st…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ba2ab27d51d4cfaa1d0273aa797cb24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%sql\n",
    "SELECT * FROM producto WHERE NombreProducto LIKE '%Pan%' limit 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_consulta = spark.sql(\"SELECT * FROM producto WHERE NombreProducto LIKE '%Pan%'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "280"
     ]
    }
   ],
   "source": [
    "df_consulta.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+\n",
      "|      NombreProducto|Producto_ID|\n",
      "+--------------------+-----------+\n",
      "|Pan Multigrano Li...|         73|\n",
      "|Pan Blanco 567g W...|         99|\n",
      "|Super Pan Bco Ajo...|        100|\n",
      "|Pan Multicereal 4...|        109|\n",
      "|Pan Multigrano 68...|        713|\n",
      "|Pan 100pct Integr...|        715|\n",
      "|Pan Blanco Siluet...|        779|\n",
      "|Panitos Chocolate...|        357|\n",
      "|Pan Bolsa 2a 500g...|       1031|\n",
      "|Pan 12 Granos 680...|       1039|\n",
      "|Panque Marmol 255...|       1064|\n",
      "|Pan Blanco Chico ...|       1109|\n",
      "|Pan Blanco 2Pq 13...|       1111|\n",
      "|Super Pan Blanco ...|       1112|\n",
      "|Pan Blanco 680g B...|       1120|\n",
      "|Pan Integral Gde ...|       1143|\n",
      "|Pan Integral 480g...|       1144|\n",
      "|Pan Integral 680g...|       1145|\n",
      "|Pan Integral 675g...|       1146|\n",
      "|Panque Pasas 255g...|       1230|\n",
      "+--------------------+-----------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "df_consulta.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Almacenamiento en una nueva colección en  mongoDB del  DataFrame resultado de la consulta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "280"
     ]
    }
   ],
   "source": [
    "df_consulta.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'20190805221113'"
     ]
    }
   ],
   "source": [
    "date_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "CadenaConexionMongo_1 = \"mongodb://127.0.0.1/bimbodb.coleccion_\" + date_time + \"?readPreference=primaryPreferred\"\n",
    "\n",
    "CadenaConexionMongo_2 = \"mongodb://127.0.0.1/bimbodb.coleccion_\" + date_time \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_consulta.write.format(\"com.mongodb.spark.sql.DefaultSource\")\\\n",
    ".mode(\"append\")\\\n",
    ".option(\"spark.mongodb.input.uri\", \"mongodb://127.0.0.1/bimbodb.coleccion_nueva2?readPreference=primaryPreferred\")\\\n",
    ".option(\"spark.mongodb.output.uri\", \"mongodb://127.0.0.1/bimbodb.coleccion_nueva2\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_consulta.write.format(\"com.mongodb.spark.sql.DefaultSource\")\\\n",
    ".mode(\"append\")\\\n",
    ".option(\"spark.mongodb.input.uri\", CadenaConexionMongo_1)\\\n",
    ".option(\"spark.mongodb.output.uri\", CadenaConexionMongo_2).save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lectura de la collección creada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_colleccion_nueva = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\")\\\n",
    "    .option(\"spark.mongodb.input.uri\", \"mongodb://127.0.0.1/bimbodb.coleccion_nueva2?readPreference=primaryPreferred\")\\\n",
    "    .option(\"spark.mongodb.output.uri\", \"mongodb://127.0.0.1/bimbodb.coleccion_nueva2\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_colleccion_nueva = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\")\\\n",
    "    .option(\"spark.mongodb.input.uri\", CadenaConexionMongo_1)\\\n",
    "    .option(\"spark.mongodb.output.uri\", CadenaConexionMongo_2).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- NombreProducto: string (nullable = true)\n",
      " |-- Producto_ID: integer (nullable = true)\n",
      " |-- _id: struct (nullable = true)\n",
      " |    |-- oid: string (nullable = true)"
     ]
    }
   ],
   "source": [
    "df_colleccion_nueva.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "280"
     ]
    }
   ],
   "source": [
    "df_colleccion_nueva.select('NombreProducto').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+--------------------+\n",
      "|      NombreProducto|Producto_ID|                 _id|\n",
      "+--------------------+-----------+--------------------+\n",
      "|Pan Multigrano Li...|         73|[5d48f07f2687dd4b...|\n",
      "|Pan Blanco 567g W...|         99|[5d48f07f2687dd4b...|\n",
      "|Super Pan Bco Ajo...|        100|[5d48f07f2687dd4b...|\n",
      "|Pan Multicereal 4...|        109|[5d48f07f2687dd4b...|\n",
      "|Pan Multigrano 68...|        713|[5d48f07f2687dd4b...|\n",
      "|Pan 100pct Integr...|        715|[5d48f07f2687dd4b...|\n",
      "|Pan Blanco Siluet...|        779|[5d48f07f2687dd4b...|\n",
      "|Panitos Chocolate...|        357|[5d48f07f2687dd4b...|\n",
      "|Pan Bolsa 2a 500g...|       1031|[5d48f07f2687dd4b...|\n",
      "|Pan 12 Granos 680...|       1039|[5d48f07f2687dd4b...|\n",
      "|Panque Marmol 255...|       1064|[5d48f07f2687dd4b...|\n",
      "|Pan Blanco Chico ...|       1109|[5d48f07f2687dd4b...|\n",
      "|Pan Blanco 2Pq 13...|       1111|[5d48f07f2687dd4b...|\n",
      "|Super Pan Blanco ...|       1112|[5d48f07f2687dd4b...|\n",
      "|Pan Blanco 680g B...|       1120|[5d48f07f2687dd4b...|\n",
      "|Pan Integral Gde ...|       1143|[5d48f07f2687dd4b...|\n",
      "|Pan Integral 480g...|       1144|[5d48f07f2687dd4b...|\n",
      "|Pan Integral 680g...|       1145|[5d48f07f2687dd4b...|\n",
      "|Pan Integral 675g...|       1146|[5d48f07f2687dd4b...|\n",
      "|Panque Pasas 255g...|       1230|[5d48f07f2687dd4b...|\n",
      "+--------------------+-----------+--------------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "df_colleccion_nueva.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark3",
   "language": "",
   "name": "pyspark3kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark3",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
