{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tópico: Hadoop Distributed File System (HDFS)\n",
    "#### Ejercicios básicos\n",
    "#### Autor: Dr. Gabriel Guerrero, saxsa2000@gmail.com\n",
    "##### Fecha: 20190702\n",
    "##### saXsa,  sistemas abiertos X sistemas abiertos S.C."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### HDFS  Hadoop Distributed File System\n",
    "\n",
    "<img src=\"Hadoop.jpg\"/>\n",
    "\n",
    "### Apache Hadoop es un ambiente de software que soporta el almacenamiento distribuido y tolerante a fallas de código abierto \n",
    "\n",
    "### Permite  trabajar con un gran número de nodos y volúmenes de datos\n",
    "\n",
    "<img src=\"hadoop_arch.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ubicacion del servidor Hadoop HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/hadoop\r\n"
     ]
    }
   ],
   "source": [
    "!echo $HADOOP_HOME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hadoop es un servidor construido con Java\n",
    "\n",
    "Para analizar el conjunto de procesos java en el equipo, se ejecuta el enunciado (jps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13668 Jps\r\n"
     ]
    }
   ],
   "source": [
    "!jps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iniciamos el servidor Hadoop HDFS por medio del guion start-dfs.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rwxr-xr-x. 1 saxsa saxsa 3734 may  7  2018 /usr/local/hadoop/sbin/start-dfs.sh\r\n"
     ]
    }
   ],
   "source": [
    "!ls  -l $HADOOP_HOME/sbin/start-dfs.sh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13681 Jps\r\n"
     ]
    }
   ],
   "source": [
    "!jps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting namenodes on [hostsaxsa]\n",
      "hostsaxsa: starting namenode, logging to /usr/local/hadoop/logs/hadoop-saxsa-namenode-hostsaxsa.out\n",
      "hostsaxsa: starting datanode, logging to /usr/local/hadoop/logs/hadoop-saxsa-datanode-hostsaxsa.out\n",
      "Starting secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: starting secondarynamenode, logging to /usr/local/hadoop/logs/hadoop-saxsa-secondarynamenode-hostsaxsa.out\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "start-dfs.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analizamos los servicios Hadoop HDFS\n",
    "\n",
    "En el caso de un pseudocluster tenemos \n",
    "\n",
    "DataNode => Servidor de almacenamiento de datos en maquina esclava\n",
    "\n",
    "NameNode => Servidor de Nombres de Archivos almacenados, maquina maestra\n",
    "\n",
    "SecondaryNameNode => Servidor Espejo de Nombres de Archivos. Es una copia de NameNode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13813 NameNode\n",
      "14167 SecondaryNameNode\n",
      "14297 Jps\n",
      "13978 DataNode\n"
     ]
    }
   ],
   "source": [
    "!jps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez iniciado el servicio Hadoop HDFS, podemos monitorear el almacenamiento en Hadoop HDFS, por medio de un navegador Web\n",
    "\n",
    "En la version actual de HDFS 2.8.4, el puerto es 50070\n",
    "\n",
    "Arrancar un navegador web en el puerto 50070\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 items\n",
      "drwxr-xr-x   - saxsa supergroup          0 2019-07-02 19:01 /FuentesCurso\n",
      "drwxr-xr-x   - saxsa supergroup          0 2019-06-22 14:25 /consulta_parquet\n",
      "drwx-wx-wx   - saxsa supergroup          0 2019-06-22 13:33 /tmp\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Analizar contenido de la carpeta \"FuentesCurso\" en \"/ (raíz)\" de Hadoop HDFS\n",
    "\n",
    "hdfs dfs -ls /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mkdir: `/FuentesCurso': File exists\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Crear carpeta \"FuentesCurso\" en \"/ (raíz)\" de Hadoop HDFS\n",
    "\n",
    "hdfs dfs -mkdir /FuentesCurso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\n",
      "-rw-r--r--   1 saxsa supergroup      46306 2019-06-22 12:55 /FuentesCurso/libro_muertos.txt\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Analizar contenido de la carpeta \"FuentesCurso\" en \"/ (raíz)\" de Hadoop HDFS\n",
    "\n",
    "hdfs dfs -ls /FuentesCurso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analisis del contenido en sistema Linux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 77788\r\n",
      "-rw-rw-r-- 1 saxsa saxsa 39765082 jun  6 12:59 2018-01.csv\r\n",
      "-rw-r--r-- 1 saxsa saxsa 39765082 jun 22 12:38 Aqui\r\n",
      "-rwxrwxr-x 1 saxsa saxsa      810 jun  6 12:59 Ejecucion.txt\r\n",
      "-rw-rw-r-- 1 saxsa saxsa    82504 jun  6 13:24 hadoop_arch.png\r\n",
      "-rw-rw-r-- 1 saxsa saxsa    12511 jun  6 12:59 Hadoop.jpg\r\n",
      "-rw-rw-r-- 1 saxsa saxsa    11839 jul  2 19:04 HDFS_HadoopDistributedFileSystem.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Subir archivo a Hadoop HDFS \n",
    "\n",
    "hdfs dfs -put ./2018-01.csv /FuentesCurso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "-rw-r--r--   1 saxsa supergroup   39765082 2019-07-02 19:09 /FuentesCurso/2018-01.csv\n",
      "-rw-r--r--   1 saxsa supergroup      46306 2019-06-22 12:55 /FuentesCurso/libro_muertos.txt\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Analizar contenido de la carpeta \"FuentesCurso\" en \"/ (raíz)\" de Hadoop HDFS\n",
    "\n",
    "hdfs dfs -ls /FuentesCurso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 72\r\n",
      "drwxrwxr-x 8 saxsa saxsa   122 jul  2 16:41 carpetaCurso\r\n",
      "-rwxr--r-- 1 saxsa saxsa  2434 may 16  2018 firefox.desktop\r\n",
      "-rw-rw-r-- 1 saxsa saxsa   306 jun 21 17:26 gg20190621_1721_Acordeon.txt\r\n",
      "-rw-rw-r-- 1 saxsa saxsa   217 jul  2 16:46 gg20190702_Acordeon_CursoCienciaDatosSparkSQLPython.txt\r\n",
      "-rwxr--r-- 1 saxsa saxsa 22687 abr 10  2018 gnome-system-monitor.desktop\r\n",
      "-rwxr--r-- 1 saxsa saxsa 22232 ago  5  2017 org.gnome.Screenshot.desktop\r\n",
      "-rwxr--r-- 1 saxsa saxsa 10410 ago  9  2017 org.gnome.Terminal.desktop\r\n"
     ]
    }
   ],
   "source": [
    "!ls  -l /home/saxsa/Escritorio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Descargar archivo de Hadoop HDFS\n",
    "\n",
    "hdfs dfs -get /FuentesCurso/2018-01.csv /home/saxsa/Escritorio/Descargado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 38908\r\n",
      "drwxrwxr-x 8 saxsa saxsa      122 jul  2 16:41 carpetaCurso\r\n",
      "-rw-r--r-- 1 saxsa saxsa 39765082 jul  2 19:09 Descargado\r\n",
      "-rwxr--r-- 1 saxsa saxsa     2434 may 16  2018 firefox.desktop\r\n",
      "-rw-rw-r-- 1 saxsa saxsa      306 jun 21 17:26 gg20190621_1721_Acordeon.txt\r\n",
      "-rw-rw-r-- 1 saxsa saxsa      217 jul  2 16:46 gg20190702_Acordeon_CursoCienciaDatosSparkSQLPython.txt\r\n",
      "-rwxr--r-- 1 saxsa saxsa    22687 abr 10  2018 gnome-system-monitor.desktop\r\n",
      "-rwxr--r-- 1 saxsa saxsa    22232 ago  5  2017 org.gnome.Screenshot.desktop\r\n",
      "-rwxr--r-- 1 saxsa saxsa    10410 ago  9  2017 org.gnome.Terminal.desktop\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l /home/saxsa/Escritorio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm /home/saxsa/Escritorio/Descargado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /FuentesCurso/2018-01.csv\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Eliminar archivo de Hadoop HDFS \n",
    "\n",
    "hdfs dfs -rm /FuentesCurso/2018-01.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rmdir: `/FuentesCurso': Directory is not empty\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Eliminar directorio /FuentesCurso de Hadoop HDFS \n",
    "\n",
    "hdfs dfs -rmdir /FuentesCurso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parar el servidor Hadoop HDFS por medio del guion stop-dfs.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rwxr-xr-x. 1 saxsa saxsa 3206 may  7  2018 /usr/local/hadoop/sbin/stop-dfs.sh\r\n"
     ]
    }
   ],
   "source": [
    "!ls  -l $HADOOP_HOME/sbin/stop-dfs.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping namenodes on [hostsaxsa]\n",
      "hostsaxsa: stopping namenode\n",
      "hostsaxsa: stopping datanode\n",
      "Stopping secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: stopping secondarynamenode\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Detener el servicio Hadoop HDFS\n",
    "\n",
    "stop-dfs.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15068 Jps\r\n"
     ]
    }
   ],
   "source": [
    "!jps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
